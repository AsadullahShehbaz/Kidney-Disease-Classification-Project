# ðŸ¥ Kidney Disease Classification - Deep Learning Project
### Complete Guide for Beginners | VGG16 CNN Classifier with MLflow & DVC

---

## ðŸ“‘ Table of Contents
1. [Project Overview](#-project-overview)
2. [Project Architecture](#-project-architecture)
3. [Directory Structure Explained](#-directory-structure-explained)
4. [Step-by-Step Workflow](#-step-by-step-workflow)
5. [Installation & Setup](#-installation--setup)
6. [Understanding Each Component](#-understanding-each-component)
7. [MLflow Integration](#-mlflow-integration)
8. [DVC Integration](#-dvc-integration)
9. [AWS Deployment](#-aws-deployment)
10. [Troubleshooting](#-troubleshooting)

---

## ðŸŽ¯ Project Overview

### What Does This Project Do?
This project classifies kidney CT scan images into two categories:
- **Normal Kidney** ðŸŸ¢
- **Diseased Kidney (Tumor/Stone/Cyst)** ðŸ”´

### Technologies Used
- **Deep Learning**: TensorFlow/Keras with VGG16 (Transfer Learning)
- **Experiment Tracking**: MLflow (tracks model performance)
- **Pipeline Management**: DVC (Data Version Control)
- **Deployment**: AWS EC2 + Docker
- **Web Interface**: Flask application

### Why This Structure?
This is a **production-grade** ML project structure used by companies like:
- Google, Amazon, Microsoft
- AI startups and consulting firms
- Professional freelance projects

**NOT just a Jupyter notebook!** This is how **real AI Engineers** build deployable systems.

---

## ðŸ—ï¸ Project Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER UPLOADS KIDNEY IMAGE                     â”‚
â”‚                            (Web UI)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLASK APP (app.py)                            â”‚
â”‚              Receives image â†’ Processes â†’ Predicts               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 TRAINED VGG16 MODEL                              â”‚
â”‚        (artifacts/training/model.h5)                             â”‚
â”‚                                                                  â”‚
â”‚  Input: 224x224 RGB Image                                       â”‚
â”‚  Output: [probability_normal, probability_diseased]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    HOW WAS THIS MODEL CREATED?
                               â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING PIPELINE                             â”‚
â”‚                                                                  â”‚
â”‚  Stage 1: Data Ingestion                                        â”‚
â”‚  â”œâ”€â”€ Download dataset from Drive/S3                             â”‚
â”‚  â””â”€â”€ Extract and organize images                                â”‚
â”‚                                                                  â”‚
â”‚  Stage 2: Prepare Base Model                                    â”‚
â”‚  â”œâ”€â”€ Load pre-trained VGG16                                     â”‚
â”‚  â”œâ”€â”€ Remove top layers                                          â”‚
â”‚  â””â”€â”€ Add custom classification head                             â”‚
â”‚                                                                  â”‚
â”‚  Stage 3: Model Training                                        â”‚
â”‚  â”œâ”€â”€ Load training data                                         â”‚
â”‚  â”œâ”€â”€ Data augmentation                                          â”‚
â”‚  â”œâ”€â”€ Train model with fine-tuning                               â”‚
â”‚  â””â”€â”€ Save trained model                                         â”‚
â”‚                                                                  â”‚
â”‚  Stage 4: Model Evaluation                                      â”‚
â”‚  â”œâ”€â”€ Test on validation set                                     â”‚
â”‚  â”œâ”€â”€ Calculate accuracy, loss                                   â”‚
â”‚  â””â”€â”€ Log results to MLflow                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 EXPERIMENT TRACKING                              â”‚
â”‚                                                                  â”‚
â”‚  MLflow: Logs metrics, parameters, models                       â”‚
â”‚  DVC: Tracks data versions and pipeline stages                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‚ Directory Structure Explained

```
Kidney-Disease-Classification/
â”‚
â”œâ”€â”€ ðŸ“ .github/                          # GitHub Actions (CI/CD automation)
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ .gitkeep                     # Keeps empty folder in Git
â”‚       â””â”€â”€ main.yaml                    # Auto-deployment workflow
â”‚
â”œâ”€â”€ ðŸ“ config/                           # âš™ï¸ CONFIGURATION FILES
â”‚   â”œâ”€â”€ config.yaml                      # Paths & directories (WHERE things are)
â”‚   â””â”€â”€ secrets.yaml                     # API keys, passwords (optional)
â”‚
â”œâ”€â”€ ðŸ“ research/                         # ðŸ§ª EXPERIMENTATION ZONE
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb         # Test data download
â”‚   â”œâ”€â”€ 02_prepare_base_model.ipynb     # Test VGG16 setup
â”‚   â”œâ”€â”€ 03_model_training.ipynb         # Test training process
â”‚   â””â”€â”€ 04_model_evaluation.ipynb       # Test evaluation
â”‚   
â”‚   # ðŸ’¡ Workflow: Experiment here â†’ Then convert to modular code
â”‚
â”œâ”€â”€ ðŸ“ src/cnnClassifier/               # ðŸ§  MAIN APPLICATION CODE
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“„ __init__.py                  # Makes this a Python package
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ components/                  # ðŸ”§ BUILDING BLOCKS (Core Logic)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py          # Downloads kidney CT images
â”‚   â”‚   â”œâ”€â”€ prepare_base_model.py      # Sets up VGG16 architecture
â”‚   â”‚   â”œâ”€â”€ model_training.py          # Trains the CNN
â”‚   â”‚   â””â”€â”€ model_evaluation.py        # Tests model accuracy
â”‚   â”‚   
â”‚   â”‚   # Each file = One responsibility
â”‚   â”‚   # Example: If download fails, check data_ingestion.py only
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ utils/                       # ðŸ› ï¸ HELPER FUNCTIONS (Reusable Tools)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ common.py                  # read_yaml(), save_json(), create_directories()
â”‚   â”‚   
â”‚   â”‚   # Used by ALL components
â”‚   â”‚   # Write once, use everywhere!
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ config/                      # ðŸŽ›ï¸ CONFIGURATION MANAGER
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ configuration.py           # Reads config.yaml & params.yaml
â”‚   â”‚   
â”‚   â”‚   # Central brain: "What settings does each component need?"
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ pipeline/                    # ðŸ”„ COMPLETE WORKFLOWS
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ stage_01_data_ingestion.py          # Pipeline: Download data
â”‚   â”‚   â”œâ”€â”€ stage_02_prepare_base_model.py      # Pipeline: Setup model
â”‚   â”‚   â”œâ”€â”€ stage_03_model_training.py          # Pipeline: Train model
â”‚   â”‚   â”œâ”€â”€ stage_04_model_evaluation.py        # Pipeline: Evaluate
â”‚   â”‚   â””â”€â”€ predict.py                          # Prediction pipeline
â”‚   â”‚   
â”‚   â”‚   # Connects components into end-to-end processes
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ entity/                      # ðŸ“‹ DATA BLUEPRINTS
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config_entity.py           # DataIngestionConfig, TrainingConfig, etc.
â”‚   â”‚   
â”‚   â”‚   # Defines structure: "What data does each component expect?"
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“ constants/                   # ðŸ”’ FIXED VALUES (Never Change)
â”‚       â””â”€â”€ __init__.py                # CONFIG_FILE_PATH, PARAMS_FILE_PATH
â”‚
â”œâ”€â”€ ðŸ“ templates/                        # ðŸŒ WEB INTERFACE
â”‚   â””â”€â”€ index.html                      # Upload image â†’ Get prediction
â”‚
â”œâ”€â”€ ðŸ“ artifacts/                        # ðŸ’¾ GENERATED OUTPUTS (Git ignored)
â”‚   â”œâ”€â”€ data_ingestion/                 # Downloaded & extracted data
â”‚   â”œâ”€â”€ prepare_base_model/             # VGG16 base model files
â”‚   â”œâ”€â”€ training/                       # Trained model (model.h5)
â”‚   â””â”€â”€ evaluation/                     # Evaluation results (scores.json)
â”‚   
â”‚   # Created automatically during training
â”‚   # Not uploaded to GitHub (too large)
â”‚
â”œâ”€â”€ ðŸ“„ config.yaml                       # âš™ï¸ PATHS & DIRECTORIES
â”œâ”€â”€ ðŸ“„ params.yaml                       # ðŸŽšï¸ MODEL HYPERPARAMETERS
â”œâ”€â”€ ðŸ“„ dvc.yaml                          # ðŸ“Š DVC PIPELINE DEFINITION
â”œâ”€â”€ ðŸ“„ requirements.txt                  # ðŸ“¦ PYTHON DEPENDENCIES
â”œâ”€â”€ ðŸ“„ setup.py                          # ðŸ“¦ MAKES PROJECT INSTALLABLE
â”œâ”€â”€ ðŸ“„ main.py                           # â–¶ï¸ TRAINING ENTRY POINT
â”œâ”€â”€ ðŸ“„ app.py                            # ðŸŒ FLASK WEB APP
â”œâ”€â”€ ðŸ“„ Dockerfile                        # ðŸ³ DOCKER IMAGE RECIPE
â”œâ”€â”€ ðŸ“„ .dvcignore                        # Ignore files for DVC
â”œâ”€â”€ ðŸ“„ .gitignore                        # Ignore files for Git
â””â”€â”€ ðŸ“„ README.md                         # ðŸ“– THIS FILE!
```

---

## ðŸ”„ Step-by-Step Workflow

### **Complete Development Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: SETUP PROJECT STRUCTURE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Run: python template.py                                         â”‚
â”‚ Creates all folders and files automatically                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: DEFINE CONFIGURATIONS                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Edit config/config.yaml:                                        â”‚
â”‚   - Where to store data? (artifacts/data_ingestion)             â”‚
â”‚   - Where to save model? (artifacts/training/model.h5)          â”‚
â”‚                                                                  â”‚
â”‚ Edit params.yaml:                                               â”‚
â”‚   - Image size: [224, 224, 3]                                   â”‚
â”‚   - Learning rate: 0.001                                        â”‚
â”‚   - Epochs: 10                                                  â”‚
â”‚   - Batch size: 16                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: EXPERIMENT IN JUPYTER                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ research/01_data_ingestion.ipynb:                               â”‚
â”‚   - Test downloading dataset                                    â”‚
â”‚   - Test extraction                                             â”‚
â”‚   - Verify data structure                                       â”‚
â”‚                                                                  â”‚
â”‚ Once working â†’ Convert to component!                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: CREATE DATA STRUCTURES                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Edit entity/config_entity.py:                                   â”‚
â”‚                                                                  â”‚
â”‚ @dataclass                                                      â”‚
â”‚ class DataIngestionConfig:                                      â”‚
â”‚     root_dir: Path                                              â”‚
â”‚     source_URL: str                                             â”‚
â”‚     local_data_file: Path                                       â”‚
â”‚     unzip_dir: Path                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: UPDATE CONFIGURATION MANAGER                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Edit config/configuration.py:                                   â”‚
â”‚                                                                  â”‚
â”‚ def get_data_ingestion_config(self):                            â”‚
â”‚     config = self.config.data_ingestion                         â”‚
â”‚     create_directories([config.root_dir])                       â”‚
â”‚     return DataIngestionConfig(...)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: BUILD COMPONENT                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Create components/data_ingestion.py:                            â”‚
â”‚                                                                  â”‚
â”‚ class DataIngestion:                                            â”‚
â”‚     def __init__(self, config):                                 â”‚
â”‚         self.config = config                                    â”‚
â”‚                                                                  â”‚
â”‚     def download_file(self):                                    â”‚
â”‚         # Download logic                                        â”‚
â”‚                                                                  â”‚
â”‚     def extract_zip_file(self):                                 â”‚
â”‚         # Extraction logic                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: CREATE PIPELINE                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Create pipeline/stage_01_data_ingestion.py:                     â”‚
â”‚                                                                  â”‚
â”‚ class DataIngestionTrainingPipeline:                            â”‚
â”‚     def main(self):                                             â”‚
â”‚         config = ConfigurationManager()                         â”‚
â”‚         data_config = config.get_data_ingestion_config()        â”‚
â”‚         data_ingestion = DataIngestion(data_config)             â”‚
â”‚         data_ingestion.download_file()                          â”‚
â”‚         data_ingestion.extract_zip_file()                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: UPDATE MAIN.PY                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Edit main.py:                                                   â”‚
â”‚                                                                  â”‚
â”‚ STAGE_NAME = "Data Ingestion"                                   â”‚
â”‚ try:                                                            â”‚
â”‚     logger.info(f">>>>> stage {STAGE_NAME} started")            â”‚
â”‚     pipeline = DataIngestionTrainingPipeline()                  â”‚
â”‚     pipeline.main()                                             â”‚
â”‚     logger.info(f">>>>> stage {STAGE_NAME} completed")          â”‚
â”‚ except Exception as e:                                          â”‚
â”‚     logger.exception(e)                                         â”‚
â”‚     raise e                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 9: REPEAT FOR ALL STAGES                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Stage 2: Prepare Base Model                                   â”‚
â”‚ - Stage 3: Model Training                                       â”‚
â”‚ - Stage 4: Model Evaluation                                     â”‚
â”‚                                                                  â”‚
â”‚ Each follows same pattern: config â†’ entity â†’ component â†’        â”‚
â”‚ pipeline â†’ main.py                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 10: SETUP DVC PIPELINE                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Edit dvc.yaml to define stage dependencies                      â”‚
â”‚ Run: dvc repro (executes entire pipeline)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 11: CREATE WEB APP                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Edit app.py (Flask application)                                 â”‚
â”‚ Create prediction endpoint                                      â”‚
â”‚ Test locally                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 12: DEPLOY TO AWS                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Dockerize application                                         â”‚
â”‚ - Push to ECR                                                   â”‚
â”‚ - Deploy on EC2                                                 â”‚
â”‚ - Setup CI/CD with GitHub Actions                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ’» Installation & Setup

### **Prerequisites:**
- Python 3.8 or higher
- Git installed
- Anaconda/Miniconda (recommended)
- 4GB+ RAM
- Internet connection

### **Step 1: Clone Repository**
```bash
git clone https://github.com/krishnaik06/Kidney-Disease-Classification-Deep-Learning-Project
cd Kidney-Disease-Classification-Deep-Learning-Project
```

### **Step 2: Create Virtual Environment**

**Option A: Using Conda (Recommended)**
```bash
# Create environment
conda create -n kidney_classifier python=3.8 -y

# Activate environment
conda activate kidney_classifier
```

**Option B: Using venv**
```bash
# Create environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate
```

### **Step 3: Install Dependencies**
```bash
pip install -r requirements.txt
```

**What gets installed?**
```txt
tensorflow==2.12.0          # Deep learning framework
pandas==2.0.3              # Data manipulation
numpy==1.24.3              # Numerical computing
matplotlib==3.7.1          # Visualization
Flask==2.3.2               # Web framework
mlflow==2.5.0              # Experiment tracking
dvc==3.15.0                # Data version control
PyYAML==6.0                # YAML file reading
python-box==7.0.1          # Dictionary with dot notation
ensure==1.0.2              # Type validation
```

### **Step 4: Initialize DVC**
```bash
dvc init
```

This creates:
- `.dvc/` folder (DVC configuration)
- `.dvcignore` (files to ignore)

---

## ðŸ§© Understanding Each Component

### **1. config.yaml - The Address Book**

```yaml
# config/config.yaml

# Root directory for all outputs
artifacts_root: artifacts

# Stage 1: Data Ingestion Configuration
data_ingestion:
  root_dir: artifacts/data_ingestion              # Where to store downloaded data
  source_URL: https://drive.google.com/file/d/1vlhZ5c7abcdef/  # Dataset URL
  local_data_file: artifacts/data_ingestion/data.zip  # Downloaded zip location
  unzip_dir: artifacts/data_ingestion             # Where to extract

# Stage 2: Base Model Preparation
prepare_base_model:
  root_dir: artifacts/prepare_base_model
  base_model_path: artifacts/prepare_base_model/base_model.h5  # Initial VGG16
  updated_base_model_path: artifacts/prepare_base_model/base_model_updated.h5  # After adding custom layers

# Stage 3: Model Training
training:
  root_dir: artifacts/training
  trained_model_path: artifacts/training/model.h5  # Final trained model

# Stage 4: Model Evaluation  
evaluation:
  root_dir: artifacts/evaluation
  mlflow_uri: https://dagshub.com/yourname/kidney-classifier.mlflow  # MLflow server
```

**ðŸ’¡ Why separate file?**
- Change paths without touching code
- Easy to switch between local/cloud storage
- Team members use same structure

---

### **2. params.yaml - The Control Panel**

```yaml
# params.yaml

# Image preprocessing
IMAGE_SIZE: [224, 224, 3]  # VGG16 requires 224x224 RGB images
BATCH_SIZE: 16             # Images per training batch
INCLUDE_TOP: False         # Remove VGG16's original classifier

# Pre-trained weights
WEIGHTS: imagenet          # Use ImageNet pre-trained weights
CLASSES: 2                 # Normal vs Diseased

# Training hyperparameters
EPOCHS: 10                 # Training iterations
LEARNING_RATE: 0.001       # How fast model learns
AUGMENTATION: True         # Apply data augmentation?

# Transfer learning strategy
FREEZE_ALL: True           # Freeze VGG16 layers?
FREEZE_TILL: null          # Or freeze specific number of layers
```

**ðŸ’¡ Experimentation made easy:**
```bash
# Try different learning rates without changing code!
# Just edit params.yaml:
LEARNING_RATE: 0.01   # Fast learning
# vs
LEARNING_RATE: 0.0001 # Slow, stable learning
```

---

### **3. Entity - Data Blueprints**

```python
# src/cnnClassifier/entity/config_entity.py

from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class DataIngestionConfig:
    """
    Configuration for data ingestion stage
    frozen=True makes it immutable (can't be changed after creation)
    """
    root_dir: Path
    source_URL: str
    local_data_file: Path
    unzip_dir: Path

@dataclass(frozen=True)
class PrepareBaseModelConfig:
    """Configuration for VGG16 model preparation"""
    root_dir: Path
    base_model_path: Path
    updated_base_model_path: Path
    params_image_size: list
    params_learning_rate: float
    params_include_top: bool
    params_weights: str
    params_classes: int

@dataclass(frozen=True)
class TrainingConfig:
    """Configuration for model training"""
    root_dir: Path
    trained_model_path: Path
    updated_base_model_path: Path
    training_data: Path
    params_epochs: int
    params_batch_size: int
    params_is_augmentation: bool
    params_image_size: list

@dataclass(frozen=True)
class EvaluationConfig:
    """Configuration for model evaluation"""
    path_of_model: Path
    training_data: Path
    all_params: dict
    mlflow_uri: str
    params_image_size: list
    params_batch_size: int
```

**ðŸ’¡ Why use dataclasses?**
```python
# âŒ Without dataclass (error-prone)
config = {
    "root_dir": "artifacts/data",
    "source_url": "https://..."  # Typo: source_url vs source_URL
}
# No error! But breaks later

# âœ… With dataclass (safe)
config = DataIngestionConfig(
    root_dir=Path("artifacts/data"),
    source_url="https://..."  # Error: unexpected keyword argument
)
# Catches mistakes immediately!
```

---

### **4. Configuration Manager - The Brain**

```python
# src/cnnClassifier/config/configuration.py

from cnnClassifier.constants import *
from cnnClassifier.utils.common import read_yaml, create_directories
from cnnClassifier.entity.config_entity import (
    DataIngestionConfig,
    PrepareBaseModelConfig,
    TrainingConfig,
    EvaluationConfig
)

class ConfigurationManager:
    """
    Central manager for all configurations
    Reads YAML files and creates config objects
    """
    def __init__(
        self,
        config_filepath = CONFIG_FILE_PATH,
        params_filepath = PARAMS_FILE_PATH
    ):
        # Read configuration files
        self.config = read_yaml(config_filepath)
        self.params = read_yaml(params_filepath)
        
        # Create root artifacts directory
        create_directories([self.config.artifacts_root])
    
    def get_data_ingestion_config(self) -> DataIngestionConfig:
        """
        Returns configuration for data ingestion stage
        """
        config = self.config.data_ingestion
        
        # Create directory for this stage
        create_directories([config.root_dir])
        
        # Create and return config object
        data_ingestion_config = DataIngestionConfig(
            root_dir=config.root_dir,
            source_URL=config.source_URL,
            local_data_file=config.local_data_file,
            unzip_dir=config.unzip_dir
        )
        
        return data_ingestion_config
    
    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:
        """Returns configuration for base model preparation"""
        config = self.config.prepare_base_model
        
        create_directories([config.root_dir])
        
        prepare_base_model_config = PrepareBaseModelConfig(
            root_dir=Path(config.root_dir),
            base_model_path=Path(config.base_model_path),
            updated_base_model_path=Path(config.updated_base_model_path),
            params_image_size=self.params.IMAGE_SIZE,
            params_learning_rate=self.params.LEARNING_RATE,
            params_include_top=self.params.INCLUDE_TOP,
            params_weights=self.params.WEIGHTS,
            params_classes=self.params.CLASSES
        )
        
        return prepare_base_model_config
    
    # Similar methods for training and evaluation configs...
```

**ðŸ’¡ How it works:**
```python
# Usage in pipeline:
config_manager = ConfigurationManager()
data_config = config_manager.get_data_ingestion_config()

print(data_config.source_URL)  # Access with dot notation
print(data_config.root_dir)
```

---

### **5. Components - The Workers**

#### **Component Example: Data Ingestion**

```python
# src/cnnClassifier/components/data_ingestion.py

import os
import urllib.request as request
import zipfile
from cnnClassifier import logger
from cnnClassifier.utils.common import get_size
from cnnClassifier.entity.config_entity import DataIngestionConfig
from pathlib import Path

class DataIngestion:
    """
    Handles downloading and extracting kidney disease dataset
    """
    def __init__(self, config: DataIngestionConfig):
        """
        Initialize with configuration
        
        Args:
            config: DataIngestionConfig object with paths and URLs
        """
        self.config = config
    
    def download_file(self):
        """
        Download dataset from Google Drive or other source
        """
        # Check if file already exists
        if not os.path.exists(self.config.local_data_file):
            logger.info("Downloading data...")
            filename, headers = request.urlretrieve(
                url=self.config.source_URL,
                filename=self.config.local_data_file
            )
            logger.info(f"Downloaded {filename} with info:\n{headers}")
        else:
            file_size = get_size(Path(self.config.local_data_file))
            logger.info(f"File already exists. Size: {file_size}")
    
    def extract_zip_file(self):
        """
        Extract downloaded zip file
        Creates: Normal/ and Tumor/ folders with images
        """
        unzip_path = self.config.unzip_dir
        os.makedirs(unzip_path, exist_ok=True)
        
        logger.info("Extracting zip file...")
        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:
            zip_ref.extractall(unzip_path)
        logger.info(f"Extracted to: {unzip_path}")
```

**ðŸ’¡ Usage:**
```python
# In pipeline:
config = ConfigurationManager()
data_config = config.get_data_ingestion_config()
data_ingestion = DataIngestion(config=data_config)

# Execute
data_ingestion.download_file()
data_ingestion.extract_zip_file()
```

---

#### **Component Example: Prepare Base Model**

```python
# src/cnnClassifier/components/prepare_base_model.py

import tensorflow as tf
from pathlib import Path
from cnnClassifier.entity.config_entity import PrepareBaseModelConfig

class PrepareBaseModel:
    """
    Downloads VGG16 and adds custom classification head
    """
    def __init__(self, config: PrepareBaseModelConfig):
        self.config = config
    
    def get_base_model(self):
        """
        Download pre-trained VGG16 model from Keras
        This model was trained on ImageNet (1.4M images)
        """
        self.model = tf.keras.applications.vgg16.VGG16(
            input_shape=self.config.params_image_size,
            weights=self.config.params_weights,  # 'imagenet'
            include_top=self.config.params_include_top  # False
        )
        
        # Save base model
        self.save_model(
            path=self.config.base_model_path,
            model=self.model
        )
    
    @staticmethod
    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):
        """
        Add custom layers on top of VGG16 for kidney classification
        
        Args:
            model: VGG16 base model
            classes: Number of output classes (2: Normal/Diseased)
            freeze_all: Freeze all VGG16 layers?
            freeze_till: Freeze layers except last n
            learning_rate: Training learning rate
        """
        # STEP 1: Freeze VGG16 layers (transfer learning)
        if freeze_all:
            # Don't update VGG16 weights during training
            for layer in model.layers:
                layer.trainable = False
        elif (freeze_till is not None) and (freeze_till > 0):
            # Freeze all except last freeze_till layers
            for layer in model.layers[:-freeze_till]:
                layer.trainable = False
        
        # STEP 2: Add custom classification head
        flatten_in = tf.keras.layers.Flatten()(model.output)
        prediction = tf.keras.layers.Dense(
            units=classes,
            activation="softmax"
        )(flatten_in)
        
        # STEP 3: Create full model
        full_model = tf.keras.models.Model(
            inputs=model.input,
            outputs=prediction
        )
        
        # STEP 4: Compile model
        full_model.compile(
            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),
            loss=tf.keras.losses.CategoricalCrossentropy(),
            metrics=["accuracy"]
        )
        
        full_model.summary()
        return full_model
    
    def update_base_model(self):
        """
        Create full model with custom head
        """
        self.full_model = self._prepare_full_model(
            model=self.model,
            classes=self.config.params_classes,
            freeze_all=True,
            freeze_till=None,
            learning_rate=self.config.params_learning_rate
        )
        
        # Save updated model
        self.save_model(
            path=self.config.updated_base_model_path,
            model=self.full_model
        )
    
    @staticmethod
    def save_model(path: Path, model: tf.keras.Model):
        """Save Keras model to disk"""
        model.save(path)
```

---

#### **Component Example: Model Training**

```python
# src/cnnClassifier/components/model_training.py

import tensorflow as tf
from pathlib import Path
from cnnClassifier.entity.config_entity import TrainingConfig

class Training:
    """
    Trains the kidney disease classifier
    """
    def __init__(self, config: TrainingConfig):
        self.config = config
    
    def get_base_model(self):
        """Load the prepared model"""
        self.model = tf.keras.models.load_model(
            self.config.updated_base_model_path
        )
    
    def train_valid_generator(self):
        """
        Create data generators for training and validation
        Applies data augmentation to increase dataset size
        """
        datagenerator_kwargs = dict(
            rescale=1./255,  # Normalize pixel values to [0,1]
            validation_split=0.20  # 20% for validation
        )
        
        dataflow_kwargs = dict(
            target_size=self.config.params_image_size[:-1],  # (224, 224)
            batch_size=self.config.params_batch_size,
            interpolation="bilinear"
        )
        
        # Validation generator (no augmentation)
        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(
            **datagenerator_kwargs
        )
        
        self.valid_generator = valid_datagenerator.flow_from_directory(
            directory=self.config.training_data,
            subset="validation",
            shuffle=False,
            **dataflow_kwargs
        )
        
        # Training generator (with augmentation if enabled)
        if self.config.params_is_augmentation:
            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(
                rotation_range=40,      # Rotate images randomly
                horizontal_flip=True,   # Flip images horizontally
                width_shift_range=0.2,  # Shift images horizontally
                height_shift_range=0.2, # Shift images vertically
                shear_range=0.2,        # Shear transformation
                zoom_range=0.2,         # Zoom in/out
                **datagenerator_kwargs
            )
        else:
            train_datagenerator = valid_datagenerator
        
        self.train_generator = train_datagenerator.flow_from_directory(
            directory=self.config.training_data,
            subset="training",
            shuffle=True,
            **dataflow_kwargs
        )
    
    def train(self):
        """
        Train the model on kidney CT scan images
        """
        # Calculate steps per epoch
        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size
        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size
        
        # Train model
        self.model.fit(
            self.train_generator,
            epochs=self.config.params_epochs,
            steps_per_epoch=self.steps_per_epoch,
            validation_steps=self.validation_steps,
            validation_data=self.valid_generator
        )
        
        # Save trained model
        self.save_model(
            path=self.config.trained_model_path,
            model=self.model
        )
    
    @staticmethod
    def save_model(path: Path, model: tf.keras.Model):
        model.save(path)
```

---

### **6. Pipeline - Connecting Everything**

```python
# src/cnnClassifier/pipeline/stage_01_data_ingestion.py

from cnnClassifier.config.configuration import ConfigurationManager
from cnnClassifier.components.data_ingestion import DataIngestion
from cnnClassifier import logger

STAGE_NAME = "Data Ingestion Stage"

class DataIngestionTrainingPipeline:
    """
    Complete pipeline for data ingestion
    Downloads and extracts kidney disease dataset
    """
    def __init__(self):
        pass
    
    def main(self):
        """Execute data ingestion pipeline"""
        # Get configuration
        config = ConfigurationManager()
        data_ingestion_config = config.get_data_ingestion_config()
        
        # Create component
        data_ingestion = DataIngestion(config=data_ingestion_config)
        
        # Execute steps
        data_ingestion.download_file()
        data_ingestion.extract_zip_file()

if __name__ == '__main__':
    try:
        logger.info(f">>>>>> stage {STAGE_NAME} started <<<<<<")
        obj = DataIngestionTrainingPipeline()
        obj.main()
        logger.info(f">>>>>> stage {STAGE_NAME} completed <<<<<<\n\nx==========x")
    except Exception as e:
        logger.exception(e)
        raise e
```

---

### **7. main.py - Orchestrating All Stages**

```python
# main.py

from cnnClassifier import logger
from cnnClassifier.pipeline.stage_01_data_ingestion import DataIngestionTrainingPipeline
from cnnClassifier.pipeline.stage_02_prepare_base_model import PrepareBaseModelTrainingPipeline
from cnnClassifier.pipeline.stage_03_model_training import ModelTrainingPipeline
from cnnClassifier.pipeline.stage_04_model_evaluation import EvaluationPipeline

# STAGE 1: Data Ingestion
STAGE_NAME = "Data Ingestion Stage"
try:
    logger.info(f">>>>>> stage {STAGE_NAME} started <<<<<<")
    data_ingestion = DataIngestionTrainingPipeline()
    data_ingestion.main()
    logger.info(f">>>>>> stage {STAGE_NAME} completed <<<<<<\n\nx==========x")
except Exception as e:
    logger.exception(e)
    raise e

# STAGE 2: Prepare Base Model
STAGE_NAME = "Prepare Base Model"
try:
    logger.info(f">>>>>> stage {STAGE_NAME} started <<<<<<")
    prepare_base_model = PrepareBaseModelTrainingPipeline()
    prepare_base_model.main()
    logger.info(f">>>>>> stage {STAGE_NAME} completed <<<<<<\n\nx==========x")
except Exception as e:
    logger.exception(e)
    raise e

# STAGE 3: Model Training
STAGE_NAME = "Training"
try:
    logger.info(f">>>>>> stage {STAGE_NAME} started <<<<<<")
    model_trainer = ModelTrainingPipeline()
    model_trainer.main()
    logger.info(f">>>>>> stage {STAGE_NAME} completed <<<<<<\n\nx==========x")
except Exception as e:
    logger.exception(e)
    raise e

# STAGE 4: Model Evaluation
STAGE_NAME = "Evaluation"
try:
    logger.info(f">>>>>> stage {STAGE_NAME} started <<<<<<")
    model_evaluation = EvaluationPipeline()
    model_evaluation.main()
    logger.info(f">>>>>> stage {STAGE_NAME} completed <<<<<<\n\nx==========x")
except Exception as e:
    logger.exception(e)
    raise e
```

---

## ðŸ“Š MLflow Integration

### **What is MLflow?**
MLflow tracks your experiments:
- What hyperparameters did you use?
- What accuracy did you get?
- Which model performed best?

### **Setup MLflow with DagsHub:**

**1. Create DagsHub Account:**
- Go to [dagshub.com](https://dagshub.com/)
- Sign up with GitHub

**2. Create Repository:**
- New Repository â†’ Connect to GitHub repo
- DagsHub creates MLflow tracking server

**3. Get Credentials:**
```
MLFLOW_TRACKING_URI=https://dagshub.com/yourusername/kidney-classifier.mlflow
MLFLOW_TRACKING_USERNAME=yourusername
MLFLOW_TRACKING_PASSWORD=your_token_here
```

**4. Set Environment Variables:**

**Windows (CMD):**
```bash
set MLFLOW_TRACKING_URI=https://dagshub.com/yourusername/kidney-classifier.mlflow
set MLFLOW_TRACKING_USERNAME=yourusername
set MLFLOW_TRACKING_PASSWORD=your_token
```

**Linux/Mac:**
```bash
export MLFLOW_TRACKING_URI=https://dagshub.com/yourusername/kidney-classifier.mlflow
export MLFLOW_TRACKING_USERNAME=yourusername
export MLFLOW_TRACKING_PASSWORD=your_token
```

**5. View MLflow UI:**
```bash
# Local UI
mlflow ui

# Open browser: http://localhost:5000
```

### **How MLflow is Used in Project:**

```python
# src/cnnClassifier/components/model_evaluation.py

import mlflow
import mlflow.keras
from urllib.parse import urlparse

class Evaluation:
    def __init__(self, config: EvaluationConfig):
        self.config = config
    
    def evaluation(self):
        """Evaluate model on validation set"""
        self.model = self.load_model(self.config.path_of_model)
        self._valid_generator()
        self.score = self.model.evaluate(self.valid_generator)
        self.save_score()
    
    def log_into_mlflow(self):
        """Log metrics and model to MLflow"""
        mlflow.set_registry_uri(self.config.mlflow_uri)
        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme
        
        with mlflow.start_run():
            # Log parameters
            mlflow.log_params(self.config.all_params)
            
            # Log metrics
            mlflow.log_metrics({
                "loss": self.score[0],
                "accuracy": self.score[1]
            })
            
            # Log model
            if tracking_url_type_store != "file":
                mlflow.keras.log_model(
                    self.model,
                    "model",
                    registered_model_name="VGG16Model"
                )
            else:
                mlflow.keras.log_model(self.model, "model")
```

**What Gets Logged:**
- âœ… Hyperparameters (learning rate, epochs, etc.)
- âœ… Metrics (accuracy, loss)
- âœ… Model file (.h5)
- âœ… Training time
- âœ… System info

---

## ðŸ“¦ DVC Integration

### **What is DVC?**
DVC (Data Version Control) is like Git for:
- Large datasets
- Model files
- ML pipelines

### **Why Use DVC?**
```
Without DVC:
âŒ Can't track which data version produced which model
âŒ Can't reproduce experiments
âŒ Large files bloat Git repository

With DVC:
âœ… Track data versions
âœ… Reproduce any experiment
âœ… Share data efficiently
âœ… Define pipeline dependencies
```

### **DVC Commands:**

```bash
# Initialize DVC
dvc init

# Track data file
dvc add artifacts/data_ingestion/data.zip

# This creates data.zip.dvc file (small, goes in Git)
# Actual data.zip is tracked by DVC

# Define pipeline in dvc.yaml
# Then run pipeline:
dvc repro

# View pipeline graph:
dvc dag
```

### **dvc.yaml Structure:**

```yaml
# dvc.yaml

stages:
  data_ingestion:
    cmd: python src/cnnClassifier/pipeline/stage_01_data_ingestion.py
    deps:
      - src/cnnClassifier/pipeline/stage_01_data_ingestion.py
      - config/config.yaml
    outs:
      - artifacts/data_ingestion/Kidney-ct-scan-image
  
  prepare_base_model:
    cmd: python src/cnnClassifier/pipeline/stage_02_prepare_base_model.py
    deps:
      - src/cnnClassifier/pipeline/stage_02_prepare_base_model.py
      - config/config.yaml
    params:
      - IMAGE_SIZE
      - INCLUDE_TOP
      - CLASSES
      - WEIGHTS
      - LEARNING_RATE
    outs:
      - artifacts/prepare_base_model
  
  training:
    cmd: python src/cnnClassifier/pipeline/stage_03_model_training.py
    deps:
      - src/cnnClassifier/pipeline/stage_03_model_training.py
      - config/config.yaml
      - artifacts/data_ingestion/Kidney-ct-scan-image
      - artifacts/prepare_base_model
    params:
      - IMAGE_SIZE
      - EPOCHS
      - BATCH_SIZE
      - AUGMENTATION
    outs:
      - artifacts/training/model.h5
  
  evaluation:
    cmd: python src/cnnClassifier/pipeline/stage_04_model_evaluation.py
    deps:
      - src/cnnClassifier/pipeline/stage_04_model_evaluation.py
      - config/config.yaml
      - artifacts/data_ingestion/Kidney-ct-scan-image
      - artifacts/training/model.h5
    params:
      - IMAGE_SIZE
      - BATCH_SIZE
    metrics:
      - scores.json:
          cache: false
```

**What This Does:**
- Defines 4 stages
- Each stage has dependencies (deps)
- When deps change, stage re-runs
- Outputs (outs) are cached
- Parameters (params) are tracked

**Run Pipeline:**
```bash
# Run all stages
dvc repro

# DVC automatically:
# - Checks what changed
# - Runs only necessary stages
# - Caches outputs

# View what will run:
dvc status
```

---

## â˜ï¸ AWS Deployment

### **Deployment Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GITHUB                                   â”‚
â”‚                    (Source Code)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ Push code
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GITHUB ACTIONS                                 â”‚
â”‚                  (CI/CD Pipeline)                                â”‚
â”‚                                                                  â”‚
â”‚  1. Run tests                                                    â”‚
â”‚  2. Build Docker image                                           â”‚
â”‚  3. Push to ECR                                                  â”‚
â”‚  4. Deploy to EC2                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS ECR                                       â”‚
â”‚            (Docker Image Registry)                               â”‚
â”‚                                                                  â”‚
â”‚  kidney-classifier:latest                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ Pull image
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS EC2                                       â”‚
â”‚                (Virtual Server)                                  â”‚
â”‚                                                                  â”‚
â”‚  Docker Container Running:                                       â”‚
â”‚  - Flask App (app.py)                                            â”‚
â”‚  - Trained Model (model.h5)                                      â”‚
â”‚  - Port 8080                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USERS                                     â”‚
â”‚              Access via: http://ec2-ip:8080                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Step-by-Step Deployment:**

#### **1. Create IAM User**

```bash
# AWS Console â†’ IAM â†’ Users â†’ Add User

User Name: kidney-classifier-deployer

Permissions:
âœ… AmazonEC2ContainerRegistryFullAccess
âœ… AmazonEC2FullAccess

# Download credentials:
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=abc123...
```

#### **2. Create ECR Repository**

```bash
# AWS Console â†’ ECR â†’ Create Repository

Repository name: kidney-classifier
Region: us-east-1

# Note the URI:
URI: 566373416292.dkr.ecr.us-east-1.amazonaws.com/kidney-classifier
```

#### **3. Create EC2 Instance**

```bash
# AWS Console â†’ EC2 â†’ Launch Instance

Name: kidney-classifier-server
AMI: Ubuntu Server 22.04 LTS
Instance type: t2.medium (4GB RAM)
Key pair: Create new (download .pem file)
Security Group:
  - Allow SSH (port 22)
  - Allow HTTP (port 80)
  - Allow Custom TCP (port 8080)

Launch instance
```

#### **4. Install Docker on EC2**

```bash
# SSH into EC2
ssh -i your-key.pem ubuntu@your-ec2-ip

# Update system
sudo apt-get update -y
sudo apt-get upgrade -y

# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Add user to docker group
sudo usermod -aG docker ubuntu

# Activate changes
newgrp docker

# Verify
docker --version
```

#### **5. Configure EC2 as Self-Hosted Runner**

```bash
# GitHub â†’ Your Repo â†’ Settings â†’ Actions â†’ Runners â†’ New self-hosted runner

# Follow instructions on EC2:
# 1. Download runner
mkdir actions-runner && cd actions-runner
curl -o actions-runner-linux-x64-2.311.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.311.0/actions-runner-linux-x64-2.311.0.tar.gz

# 2. Extract
tar xzf ./actions-runner-linux-x64-2.311.0.tar.gz

# 3. Configure
./config.sh --url https://github.com/yourusername/kidney-classifier --token YOUR_TOKEN

# 4. Install as service
sudo ./svc.sh install
sudo ./svc.sh start
```

#### **6. Setup GitHub Secrets**

```bash
# GitHub â†’ Your Repo â†’ Settings â†’ Secrets â†’ Actions â†’ New repository secret

Add these secrets:
AWS_ACCESS_KEY_ID: AKIA...
AWS_SECRET_ACCESS_KEY: abc123...
AWS_REGION: us-east-1
AWS_ECR_LOGIN_URI: 566373416292.dkr.ecr.us-east-1.amazonaws.com
ECR_REPOSITORY_NAME: kidney-classifier
```

#### **7. Create GitHub Actions Workflow**

```yaml
# .github/workflows/main.yaml

name: Deploy to AWS

on:
  push:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: self-hosted
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Login to ECR
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin ${{ secrets.AWS_ECR_LOGIN_URI }}
      
      - name: Build Docker image
        run: |
          docker build -t ${{ secrets.ECR_REPOSITORY_NAME }}:latest .
      
      - name: Tag Docker image
        run: |
          docker tag ${{ secrets.ECR_REPOSITORY_NAME }}:latest ${{ secrets.AWS_ECR_LOGIN_URI }}/${{ secrets.ECR_REPOSITORY_NAME }}:latest
      
      - name: Push to ECR
        run: |
          docker push ${{ secrets.AWS_ECR_LOGIN_URI }}/${{ secrets.ECR_REPOSITORY_NAME }}:latest
      
      - name: Pull and run on EC2
        run: |
          docker pull ${{ secrets.AWS_ECR_LOGIN_URI }}/${{ secrets.ECR_REPOSITORY_NAME }}:latest
          docker stop kidney-classifier || true
          docker rm kidney-classifier || true
          docker run -d -p 8080:8080 --name kidney-classifier ${{ secrets.AWS_ECR_LOGIN_URI }}/${{ secrets.ECR_REPOSITORY_NAME }}:latest
```

#### **8. Create Dockerfile**

```dockerfile
# Dockerfile

FROM python:3.8-slim-buster

# Set working directory
WORKDIR /app

# Copy files
COPY . /app

# Install dependencies
RUN pip install -r requirements.txt

# Expose port
EXPOSE 8080

# Run app
CMD ["python3", "app.py"]
```

#### **9. Deploy!**

```bash
# Push code to GitHub
git add .
git commit -m "Setup deployment"
git push origin main

# GitHub Actions automatically:
# 1. Builds Docker image
# 2. Pushes to ECR
# 3. Deploys to EC2

# Access app:
# http://your-ec2-ip:8080
```

---

## ðŸ› Troubleshooting

### **Common Errors & Solutions:**

#### **Error 1: `BoxKeyError: 'artifact_roots'`**
```python
# Problem: Typo in config.yaml

# Solution: Check config.yaml first line
artifact_roots: artifacts  # Must be exactly this
```

#### **Error 2: `ModuleNotFoundError: No module named 'cnnClassifier'`**
```bash
# Problem: Package not installed

# Solution: Install in editable mode
pip install -e .
```

#### **Error 3: `OOM (Out of Memory) during training`**
```yaml
# Problem: Batch size too large

# Solution: Reduce in params.yaml
BATCH_SIZE: 8  # Instead of 16 or 32
```

#### **Error 4: `Unable to download data`**
```python
# Problem: Google Drive link requires authentication

# Solution: Use direct download link
# OR manually download and place in artifacts/data_ingestion/
```

#### **Error 5: `Docker build fails`**
```bash
# Problem: Large model file in image

# Solution: Use .dockerignore
# Create .dockerignore:
research/
*.ipynb
.git/
```

#### **Error 6: `MLflow connection error`**
```bash
# Problem: Environment variables not set

# Solution: Export variables before running
export MLFLOW_TRACKING_URI=https://dagshub.com/...
export MLFLOW_TRACKING_USERNAME=...
export MLFLOW_TRACKING_PASSWORD=...
```

---

## ðŸŽ“ Learning Path for Beginners

### **Week 1-2: Understanding Structure**
- âœ… Read this README fully
- âœ… Understand each file's purpose
- âœ… Run the project locally
- âœ… Experiment in research/ notebooks

### **Week 3-4: Modify Components**
- âœ… Change hyperparameters in params.yaml
- âœ… Try different learning rates
- âœ… Modify data augmentation
- âœ… Track experiments with MLflow

### **Week 5-6: Build Your Own**
- âœ… Use this structure for a new project
- âœ… Change to different dataset (cats/dogs, flowers, etc.)
- âœ… Modify model (try ResNet50 instead of VGG16)
- âœ… Deploy your own model

### **Week 7-8: Advanced**
- âœ… Add more evaluation metrics
- âœ… Implement early stopping
- âœ… Try transfer learning fine-tuning
- âœ… Create API endpoints

---

## ðŸ“š Additional Resources

### **Learn More:**
- **TensorFlow Tutorial**: https://www.tensorflow.org/tutorials
- **MLflow Documentation**: https://mlflow.org/docs/latest/
- **DVC Tutorial**: https://dvc.org/doc/start
- **Flask Tutorial**: https://flask.palletsprojects.com/

### **Similar Projects to Practice:**
1. **Plant Disease Classification** (same structure)
2. **Chest X-Ray Classification** (COVID detection)
3. **Skin Cancer Classification** (melanoma detection)
4. **Brain Tumor Classification** (MRI images)

---

## ðŸŽ¯ Key Takeaways

### **What Makes This Project Professional:**

âœ… **Modular Code**: Each component does one thing well
âœ… **Configuration Management**: Easy to change settings
âœ… **Experiment Tracking**: Know what works (MLflow)
âœ… **Version Control**: Track data and code (Git + DVC)
âœ… **Reproducibility**: Anyone can reproduce results
âœ… **Deployment Ready**: Docker + AWS + CI/CD
âœ… **Scalable**: Easy to add features

### **Why This Structure Matters:**

**In Interviews:**
```
Interviewer: "Tell me about your projects"

âŒ Bad: "I built a CNN in Jupyter that got 95% accuracy"

âœ… Good: "I built a production-ready kidney disease classifier with:
- Modular pipeline architecture
- MLflow experiment tracking
- DVC for data versioning  
- Docker containerization
- AWS deployment with CI/CD
- 95% accuracy on validation set"
```

**In Jobs:**
```
Manager: "We need to add a new feature"

With modular structure:
âœ… "Sure, I'll add a new component and pipeline stage"
   (Takes 2-3 hours)

Without structure:
âŒ "I need to rewrite the entire notebook..."
   (Takes 2-3 days and might break everything)
```

---

## ðŸ† Final Words

**Congratulations!** You've learned a production-grade ML project structure. This is **exactly** how companies like Google, Amazon, and AI startups build ML systems.

**Remember:**
- ðŸ““ Jupyter for **experiments**
- ðŸ—ï¸ Modular code for **production**
- ðŸš€ Both skills make you **hirable**

**You're now ahead of 90% of ML students** who only know notebooks!

Keep building, keep learning! ðŸ’ªðŸš€

---

**Questions? Issues?**
- Open an issue on GitHub
- Contact instructor
- Check troubleshooting section

**Happy Coding!** ðŸŽ‰