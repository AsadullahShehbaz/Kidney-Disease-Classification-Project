{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc05b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\Codes\\\\02 MLOPs\\\\00 MLOPs Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74bca23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\Codes\\\\02 MLOPs\\\\00 MLOPs Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320afc5",
   "metadata": {},
   "source": [
    "# DataClass (Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3d10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Docstring for TrainingConfig\n",
    "    \"\"\"\n",
    "    root_dir : Path \n",
    "    trained_model_path : Path\n",
    "    updated_base_model_path : Path\n",
    "    training_data : Path\n",
    "    params_epochs : int \n",
    "    params_batch_size : int \n",
    "    params_is_augmentation : bool \n",
    "    params_image_size : list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf037b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml,create_directories\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396fb00",
   "metadata": {},
   "source": [
    "# ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3c4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_PATH_YAML,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_training_config(self)-> TrainingConfig:\n",
    "\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model \n",
    "        params = self.params \n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir,\"kidney-ct-scan-image\")\n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "        )\n",
    "\n",
    "        return training_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d1d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request as request \n",
    "from zipfile import ZipFile \n",
    "import tensorflow as tf\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28f1ea",
   "metadata": {},
   "source": [
    "# Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d1c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \"\"\"\n",
    "    Handles the complete training process for the kidney disease classifier\n",
    "    \n",
    "    This class takes care of:\n",
    "    1. Loading the prepared VGG16 model\n",
    "    2. Creating data generators for training and validation\n",
    "    3. Training the model on kidney CT scan images\n",
    "    4. Saving the trained model \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"\n",
    "        Initialize the Training class with configuration settings\n",
    "        \n",
    "        Args:\n",
    "            config: TrainingConfig object containing all training parameters like:\n",
    "                   - paths to data and model\n",
    "                   - epochs, batch size\n",
    "                   - image size\n",
    "                   - augmentation settings\n",
    "        \"\"\"\n",
    "        self.config = config \n",
    "    \n",
    "    def get_base_model(self):\n",
    "        \"\"\"\n",
    "        Load the prepared VGG16 model from disk\n",
    "        \n",
    "        This model was created in the previous stage (prepare_base_model)\n",
    "        It already has:\n",
    "        - Pre-trained VGG16 layers (frozen)\n",
    "        - Custom classification head added on top\n",
    "        - Compiled and ready for training\n",
    "        \n",
    "        The model is loaded from: artifacts/prepare_base_model/base_model_updated.h5\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            self.config.updated_base_model_path\n",
    "        )\n",
    "    \n",
    "    def train_valid_generator(self):\n",
    "        \"\"\"\n",
    "        Create data generators for feeding images to the model during training\n",
    "        \n",
    "        Data generators:\n",
    "        - Load images from disk in batches (not all at once - saves memory!)\n",
    "        - Apply preprocessing (normalization, resizing)\n",
    "        - Apply data augmentation to training set (increases dataset variety)\n",
    "        - Split data into training (80%) and validation (20%) sets\n",
    "        \n",
    "        Why use generators?\n",
    "        - Memory efficient: Only loads one batch at a time\n",
    "        - Real-time augmentation: Creates new variations on-the-fly\n",
    "        - Automatic batching: No need to manually create batches\n",
    "        \"\"\"\n",
    "        \n",
    "        # ==================== COMMON SETTINGS FOR BOTH GENERATORS ====================\n",
    "        \n",
    "        # Dictionary of settings applied to BOTH training and validation generators\n",
    "        datagenerator_kwargs = dict(\n",
    "            # Rescale pixel values from [0, 255] to [0, 1]\n",
    "            # Neural networks work better with normalized inputs\n",
    "            # Example: pixel value 255 becomes 1.0, value 127 becomes 0.498\n",
    "            rescale = 1./255,\n",
    "            \n",
    "            # Split dataset: 80% training, 20% validation\n",
    "            # Validation set is used to check if model is overfitting\n",
    "            validation_split = 0.20\n",
    "        )\n",
    "\n",
    "        # Dictionary of settings for how images flow through the generators\n",
    "        dataflow_kwargs = dict(\n",
    "            # Resize all images to (224, 224) - VGG16's required input size\n",
    "            # [:-1] removes the last element (channels), so [224, 224, 3] becomes [224, 224]\n",
    "            target_size = self.config.params_image_size[:-1],\n",
    "            \n",
    "            # Number of images to load in each batch\n",
    "            # Smaller batch = less memory, but noisier training\n",
    "            # Larger batch = more memory, but stabler training\n",
    "            # 16 is a good balance for most systems\n",
    "            batch_size = self.config.params_batch_size,\n",
    "            \n",
    "            # Method for resizing images\n",
    "            # \"bilinear\" = smooth interpolation, good quality\n",
    "            # Other options: \"nearest\", \"bicubic\"\n",
    "            interpolation = \"bilinear\"\n",
    "        )\n",
    "\n",
    "        # ==================== VALIDATION GENERATOR ====================\n",
    "        \n",
    "        # Create validation data generator\n",
    "        # Validation images get NO augmentation - we want to test on real, unchanged images\n",
    "        # Only rescaling is applied (normalize to [0, 1])\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs  # ** unpacks the dictionary into keyword arguments\n",
    "        )\n",
    "\n",
    "        # Create the actual generator that loads validation images\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            # Path to data folder containing Normal/ and Tumor/ subfolders\n",
    "            directory=self.config.training_data,\n",
    "            \n",
    "            # Use the validation split (20% of data)\n",
    "            subset=\"validation\",\n",
    "            \n",
    "            # Don't shuffle validation data - we want consistent evaluation\n",
    "            # Shuffling would give slightly different accuracy each time\n",
    "            shuffle=False, \n",
    "            \n",
    "            # Apply the dataflow settings (target_size, batch_size, etc.)\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "        # Output example: \"Found 40 images belonging to 2 classes.\"\n",
    "\n",
    "        # ==================== TRAINING GENERATOR ====================\n",
    "        \n",
    "        # Check if data augmentation is enabled in params.yaml\n",
    "        if self.config.params_is_augmentation:\n",
    "            # DATA AUGMENTATION: Create artificial variations of training images\n",
    "            # This helps prevent overfitting and makes model more robust\n",
    "            \n",
    "            # Why augmentation?\n",
    "            # - Increases effective dataset size (1000 images → 10,000+ variations)\n",
    "            # - Model learns to recognize kidneys from different angles, positions\n",
    "            # - Prevents memorization of training images\n",
    "            \n",
    "            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                # Randomly rotate images up to 40 degrees left or right\n",
    "                # Example: Original image → Rotated 15° clockwise\n",
    "                # Helps model recognize kidneys at different orientations\n",
    "                rotation_range=40,\n",
    "                \n",
    "                # Randomly flip images horizontally (left ↔ right)\n",
    "                # Example: Kidney on left side → Kidney on right side\n",
    "                # Medical images can be from either kidney!\n",
    "                horizontal_flip=True,\n",
    "                \n",
    "                # Randomly shift image horizontally by up to 20% of width\n",
    "                # Example: 224px image → shift up to 45 pixels left/right\n",
    "                # Helps model handle kidneys not perfectly centered\n",
    "                width_shift_range=0.2,\n",
    "                \n",
    "                # Randomly shift image vertically by up to 20% of height\n",
    "                # Example: 224px image → shift up to 45 pixels up/down\n",
    "                height_shift_range=0.2,\n",
    "                \n",
    "                # Apply shear transformation (slanting effect)\n",
    "                # Example: Rectangle → Parallelogram\n",
    "                # Range: 0.2 means up to 20% shear\n",
    "                # Helps with images taken at angles\n",
    "                shear_range=0.2,\n",
    "                \n",
    "                # Randomly zoom in/out by up to 20%\n",
    "                # Example: Zoom in 10% → closer view of kidney\n",
    "                # Helps model recognize kidneys at different scales\n",
    "                zoom_range=0.2,\n",
    "                \n",
    "                # Also apply rescaling and validation_split\n",
    "                **datagenerator_kwargs\n",
    "            )\n",
    "            \n",
    "            # IMPORTANT: Each image is augmented RANDOMLY each epoch\n",
    "            # Same image looks different every time it's fed to the model!\n",
    "            # This creates essentially unlimited training variations\n",
    "            \n",
    "        else:\n",
    "            # If augmentation is disabled, use same generator as validation\n",
    "            # Training images only get rescaled, no transformations\n",
    "            # Useful for: debugging, fast prototyping, or very large datasets\n",
    "            train_datagenerator = valid_datagenerator\n",
    "    \n",
    "        # Create the actual generator that loads training images\n",
    "        self.train_generator = train_datagenerator.flow_from_directory(\n",
    "            # Path to data folder\n",
    "            directory=self.config.training_data,\n",
    "            \n",
    "            # Use the training split (80% of data)\n",
    "            subset=\"training\",\n",
    "            \n",
    "            # Shuffle training data - important for good learning!\n",
    "            # Model sees images in different order each epoch\n",
    "            # Prevents learning based on image order\n",
    "            shuffle=True,\n",
    "            \n",
    "            # Apply the dataflow settings\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "        # Output example: \"Found 160 images belonging to 2 classes.\"\n",
    "        \n",
    "        # FINAL RESULT:\n",
    "        # self.train_generator: Loads augmented training images in batches\n",
    "        # self.valid_generator: Loads original validation images in batches\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        \"\"\"\n",
    "        Save the trained Keras model to disk\n",
    "        \n",
    "        @staticmethod means this method doesn't need access to self\n",
    "        It's a utility function that can be called independently\n",
    "        \n",
    "        Args:\n",
    "            path: Where to save the model (e.g., artifacts/training/model.h5)\n",
    "            model: The trained Keras model object\n",
    "        \n",
    "        Saved format: HDF5 (.h5 file)\n",
    "        Contains:\n",
    "        - Model architecture (layers, connections)\n",
    "        - Trained weights (learned parameters)\n",
    "        - Optimizer state (for resuming training)\n",
    "        - Compilation settings (loss function, metrics)\n",
    "        \n",
    "        This .h5 file can be loaded later for predictions or further training\n",
    "        \"\"\"\n",
    "        model.save(path)\n",
    "\n",
    "    # def train(self, callback_list: list):\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the VGG16 model on kidney CT scan images\n",
    "        \n",
    "        This is the main training loop that:\n",
    "        1. Feeds images to the model batch by batch\n",
    "        2. Model makes predictions\n",
    "        3. Calculates loss (how wrong predictions are)\n",
    "        4. Updates weights to improve\n",
    "        5. Repeats for multiple epochs\n",
    "        6. Saves the trained model\n",
    "        \n",
    "        Args:\n",
    "            callback_list: List of Keras callbacks (e.g., early stopping, model checkpointing)\n",
    "                          Callbacks are functions called during training to:\n",
    "                          - Save best model\n",
    "                          - Stop if not improving\n",
    "                          - Log metrics to MLflow\n",
    "                          - Reduce learning rate\n",
    "        \"\"\"\n",
    "        \n",
    "        # ==================== CALCULATE TRAINING STEPS ====================\n",
    "        \n",
    "        # Calculate how many batches (steps) in one epoch for TRAINING data\n",
    "        # Formula: total_images // batch_size\n",
    "        # Example: 160 training images, batch_size=16 → 160 // 16 = 10 steps per epoch\n",
    "        # \n",
    "        # Why // (floor division)?\n",
    "        # - If 165 images and batch_size=16 → 165 // 16 = 10 steps\n",
    "        # - Last 5 images are dropped (can't form complete batch)\n",
    "        # - This ensures all batches have exactly the same size\n",
    "        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size \n",
    "        \n",
    "        # Calculate how many batches (steps) for VALIDATION data\n",
    "        # Example: 40 validation images, batch_size=16 → 40 // 16 = 2 steps\n",
    "        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size \n",
    "        \n",
    "        # ==================== TRAIN THE MODEL ====================\n",
    "        \n",
    "        # model.fit() is the main training function\n",
    "        # It runs the entire training loop:\n",
    "        # For each epoch:\n",
    "        #   For each training batch:\n",
    "        #     1. Forward pass: predict → calculate loss\n",
    "        #     2. Backward pass: calculate gradients\n",
    "        #     3. Update weights\n",
    "        #   For each validation batch:\n",
    "        #     1. Predict (no weight updates)\n",
    "        #     2. Calculate metrics\n",
    "        \n",
    "        self.model.fit(\n",
    "            # Training data generator\n",
    "            # Provides batches of (images, labels) automatically\n",
    "            # Example batch: 16 images of shape (224, 224, 3) + 16 labels [0 or 1]\n",
    "            self.train_generator,\n",
    "            \n",
    "            # Number of times to iterate over the ENTIRE dataset\n",
    "            # Example: epochs=10 means model sees each image 10 times\n",
    "            # (Actually more due to augmentation - each time looks different!)\n",
    "            # \n",
    "            # What happens each epoch:\n",
    "            # Epoch 1: Model is terrible (random weights)\n",
    "            # Epoch 5: Model is learning patterns\n",
    "            # Epoch 10: Model is good at recognizing kidneys\n",
    "            # Epoch 20+: Might start overfitting (memorizing instead of learning)\n",
    "            epochs = self.config.params_epochs,\n",
    "            \n",
    "            # Number of batches to process in one epoch\n",
    "            # We calculated this above: total_training_images // batch_size\n",
    "            # Example: 10 steps means model processes 10 batches per epoch\n",
    "            steps_per_epoch = self.steps_per_epoch,\n",
    "            \n",
    "            # Number of validation batches to process after each epoch\n",
    "            # Used to check model performance on unseen data\n",
    "            # Example: 2 steps means process 2 batches of validation data\n",
    "            validation_steps = self.validation_steps,\n",
    "            \n",
    "            # Validation data generator\n",
    "            # After each epoch, model is evaluated on this data\n",
    "            # This gives us validation accuracy and loss\n",
    "            # If validation loss increases → model is overfitting!\n",
    "            validation_data = self.valid_generator,\n",
    "            \n",
    "            # Callbacks are executed at specific points during training\n",
    "            # Common callbacks:\n",
    "            # - ModelCheckpoint: Save model when validation accuracy improves\n",
    "            # - EarlyStopping: Stop training if no improvement for N epochs\n",
    "            # - TensorBoard: Log metrics for visualization\n",
    "            # - MLflowCallback: Log experiments to MLflow\n",
    "            # - ReduceLROnPlateau: Reduce learning rate if stuck\n",
    "            # callbacks = callback_list\n",
    "        )\n",
    "        \n",
    "        # TRAINING OUTPUT EXAMPLE:\n",
    "        # Epoch 1/10\n",
    "        # 10/10 [======] - 45s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
    "        # Epoch 2/10  \n",
    "        # 10/10 [======] - 42s - loss: 0.5234 - accuracy: 0.7500 - val_loss: 0.4523 - val_accuracy: 0.8000\n",
    "        # ...\n",
    "        # Epoch 10/10\n",
    "        # 10/10 [======] - 41s - loss: 0.1234 - accuracy: 0.9500 - val_loss: 0.1567 - val_accuracy: 0.9250\n",
    "        \n",
    "        # ==================== SAVE TRAINED MODEL ====================\n",
    "        \n",
    "        # After training is complete, save the final model\n",
    "        # This model now has updated weights and can classify kidney images\n",
    "        # File size: ~100-500 MB (VGG16 is large!)\n",
    "        self.save_model(\n",
    "            path = self.config.trained_model_path,  # artifacts/training/model.h5\n",
    "            model = self.model  \n",
    "        )\n",
    "        \n",
    "        # Success! The model is now trained and saved\n",
    "        # Next steps:\n",
    "        # 1. Load this model for evaluation\n",
    "        # 2. Test on new kidney images\n",
    "        # 3. Deploy as web app (app.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ead218",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538e95f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-09 10:21:21,452: INFO: common: YAML file loaded successfully: config\\config.yaml:]\n",
      "[2026-02-09 10:21:21,462: INFO: common: YAML file loaded successfully: params.yaml:]\n",
      "[2026-02-09 10:21:21,470: INFO: common: Directory created at: artifacts:]\n",
      "[2026-02-09 10:21:21,472: INFO: common: Directory created at: artifacts\\training:]\n",
      "[2026-02-09 10:21:22,245: WARNING: saving_utils: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.:]\n",
      "Found 93 images belonging to 2 classes.\n",
      "Found 372 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <Variable path=dense/kernel, shape=(25088, 2), dtype=float32>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     training.train()\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     training.get_base_model()\n\u001b[32m      7\u001b[39m     training.train_valid_generator()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 268\u001b[39m, in \u001b[36mTraining.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28mself\u001b[39m.validation_steps = \u001b[38;5;28mself\u001b[39m.valid_generator.samples // \u001b[38;5;28mself\u001b[39m.valid_generator.batch_size \n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# ==================== TRAIN THE MODEL ====================\u001b[39;00m\n\u001b[32m    256\u001b[39m \n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# model.fit() is the main training function\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    265\u001b[39m \u001b[38;5;66;03m#     1. Predict (no weight updates)\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m#     2. Calculate metrics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Training data generator\u001b[39;49;00m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Provides batches of (images, labels) automatically\u001b[39;49;00m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Example batch: 16 images of shape (224, 224, 3) + 16 labels [0 or 1]\u001b[39;49;00m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Number of times to iterate over the ENTIRE dataset\u001b[39;49;00m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Example: epochs=10 means model sees each image 10 times\u001b[39;49;00m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# (Actually more due to augmentation - each time looks different!)\u001b[39;49;00m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# \u001b[39;49;00m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# What happens each epoch:\u001b[39;49;00m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Epoch 1: Model is terrible (random weights)\u001b[39;49;00m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Epoch 5: Model is learning patterns\u001b[39;49;00m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Epoch 10: Model is good at recognizing kidneys\u001b[39;49;00m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Epoch 20+: Might start overfitting (memorizing instead of learning)\u001b[39;49;00m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Number of batches to process in one epoch\u001b[39;49;00m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# We calculated this above: total_training_images // batch_size\u001b[39;49;00m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Example: 10 steps means model processes 10 batches per epoch\u001b[39;49;00m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Number of validation batches to process after each epoch\u001b[39;49;00m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Used to check model performance on unseen data\u001b[39;49;00m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Example: 2 steps means process 2 batches of validation data\u001b[39;49;00m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Validation data generator\u001b[39;49;00m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# After each epoch, model is evaluated on this data\u001b[39;49;00m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# This gives us validation accuracy and loss\u001b[39;49;00m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If validation loss increases → model is overfitting!\u001b[39;49;00m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Callbacks are executed at specific points during training\u001b[39;49;00m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Common callbacks:\u001b[39;49;00m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# - ModelCheckpoint: Save model when validation accuracy improves\u001b[39;49;00m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# - EarlyStopping: Stop training if no improvement for N epochs\u001b[39;49;00m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# - TensorBoard: Log metrics for visualization\u001b[39;49;00m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# - MLflowCallback: Log experiments to MLflow\u001b[39;49;00m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# - ReduceLROnPlateau: Reduce learning rate if stuck\u001b[39;49;00m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks = callback_list\u001b[39;49;00m\n\u001b[32m    309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# TRAINING OUTPUT EXAMPLE:\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# Epoch 1/10\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[38;5;66;03m# 10/10 [======] - 45s - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# This model now has updated weights and can classify kidney images\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# File size: ~100-500 MB (VGG16 is large!)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;28mself\u001b[39m.save_model(\n\u001b[32m    326\u001b[39m     path = \u001b[38;5;28mself\u001b[39m.config.trained_model_path,  \u001b[38;5;66;03m# artifacts/training/model.h5\u001b[39;00m\n\u001b[32m    327\u001b[39m     model = \u001b[38;5;28mself\u001b[39m.model  \n\u001b[32m    328\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Asadullah Core\\Apps\\AS\\envs\\tf_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Asadullah Core\\Apps\\AS\\envs\\tf_env\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:408\u001b[39m, in \u001b[36mBaseOptimizer._check_variables_are_known\u001b[39m\u001b[34m(self, variables)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainable_variables_indices:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    409\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This optimizer can only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    410\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbe called for the variables it was originally built with. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    411\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mWhen working with a new set of variables, you should \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrecreate a new optimizer instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    413\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Unknown variable: <Variable path=dense/kernel, shape=(25088, 2), dtype=float32>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "# Define Pipeline \n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "except Exception as e:\n",
    "    raise e "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
